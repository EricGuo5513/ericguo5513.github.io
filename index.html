<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=windows-1252" charset="utf-8">
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>
<title>Chuan Guo - Research Scientist, Snap Research </title>
<style type="text/css"></style>
</head>

<body>
    <table border="0" width="980px" align="center">
        <tbody>
            <tr>
                <td valign="top">
                    <br>
                    <table style="font-size: 11pt;" border="0" width="100%">
                        <tbody>
                            <tr>
                                <td width="50%" align="center">
                                    <!-- <img width="300" src="./index_files/mypic.jpeg" border="0"> -->
                                    <!-- <img width="300" src="./index_files/mypic2.jpg" border="0"> -->
                                    <img width="250" src="./source_files/selfie.JPG" border="0">
                                </td>
                                <td>
                                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="6"> 
                                        <b>Chuan Guo</b><br><br>
                                    </font>
                                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="4"> 
                                        Research Scientist<br>
                                        Snap Research<br>
                                        New York City, NY<br><br>
                                        guochuan5513 at gmail.com<br><br>
                                        [<a href="https://github.com/EricGuo5513" border="0">GitHub</a>]
                                        [<a href="https://scholar.google.com/citations?user=eCdqvJoAAAAJ&hl=en" border="0">Google Scholar</a>]
                                        [<a href="https://www.linkedin.com/in/chuan-guo-59b6a810a/" border="0">Linkedin</a>]
                                        [<a href="./source_files/Chuan_Guo_CV.pdf" border="0">Download CV</a>]
                                        <!-- [<a href="index_files/CV.pdf" border="0">Resume/CV</a>]<br> -->
                                    </font> 
                                    <br><br>
                                </td>
                            </tr>
                        </tbody>
                    </table> 

                    <hr size="2" align="left" noshade="">

                    <font face="helvetica, ariel, &#39;sans serif&#39;"> 
                        I am a research scientist at <a href="https://research.snap.com/team/computational-imaging.html">Snap Research</a>. My research interests are in generative AI for digital human performance and character animation, focusing on 3D avatar animation, motion synthesis/stylization, and human-scene/object interaction. I earned my Ph.D. from the University of Alberta, advised by <a href="https://www.ece.ualberta.ca/~lcheng5/" >Prof.Li Cheng</a>. Previously, I had two-year research experience in the Multimedia Group at Institute of Computing Technology, Chinese Academy of Sciences. I received my bachelor's degree in Software Engigeering from Jilin University.

                        <br><br>
                        ðŸ”¥ Our team is hiring research interns on human interaction, motion synthesis and character animation. I am also open to academic collaboration. Contact me if you are interested. 
                    </font>

                    <!-- <br><br> -->
                    <hr size="2" align="left" noshade="">

                    <font face="helvetica, ariel, &#39;sans serif&#39;">
                        <h3>News ðŸ”ˆ</h3>

                        <span style="font-size: 10pt;">
                            <b>[Jan 2025]</b> ðŸ”¥ðŸ”¥ðŸ”¥ We are running the workshop of <a href="https://humogen.github.io/">human motion generation (HuMoGen)</a> at <b>CVPR 2025</b>. Paper submissions are welcome.<br>
                            <b>[Feb 2025]</b> I am invited for a talk about <i>generative motion modeling in discrete space</i> at Northeastern University.<br>

                            <b>[Jan 2025]</b> Two works (<a href="https://gohar-malik.github.io/">InterMask</a> and MotionDreamer) are accepted to ICLR 2025! Congratulations to <a href="https://vision-and-learning-lab-ualberta.github.io/author/yilin-wang/">Yilin</a> and <a href="https://gohar-malik.github.io/">Gohar!</a></b>.<br>

                            <b>[July 2024]</b> Our work on Hand Grasp Generation was accepted to <b><a href="https://hands-workshop.org/">ECCV Workshop 2024</a></b>.<br>
                            <b>[July 2024]</b> Our work on <a href="https://yxmu.foo/GSD/">3D Gaussian splatting diffusion</a> was accepted to <b>ECCV 2024</b>.<br>

                            <!-- <b>[June 2024]</b> We were running the first workshop of <a href="https://humogen.github.io/">human motion generation (HuMoGen)</a> at <b>CVPR 2024</b>.<br> -->
                            <!-- <b>[June 2024]</b> I am attending CVPR 2024 at Seattle. Feel free to let me know if you want a connect.<br> -->
                            <b>[May  2024]</b> I joined <a href="https://research.snap.com/team/category/computational-imaging.html">Snap Research</a> as a Research Scientist.<br>
                            <b>[Feb  2024]</b> Our <a href="https://ericguo5513.github.io/momask"><b>MoMask</b></a> was accepted to <b>CVPR 2024</b>.<br>
                            <b>[Jan  2024]</b> I have successfully defended my PhD theis.</a><br>
                            <b>[Jan  2024]</b> Our work on <a href="https://yxmu.foo/GenMoStyle/">generative motion stylization</a> was accepted to <b>ICLR 2024</b>.<br>
                            <b>[Dec  2023]</b> I was invited for a talk about human action analysis at <a href="https://www.youtube.com/watch?v=dAWXwZlwjC4">Amii AI Seminar.</a><br>
                            <b>[Dec  2023]</b> Our work, <a href="https://nhathoang2002.github.io/MotionMix-page/">MotionMix</a>, working on weakly-supervised 3D human motion generation was accepted to <b>AAAI 2024</b>.<br>
                            <b>[Dec  2023]</b> Check out our new state-of-the-art motion generation models, <a href="https://ericguo5513.github.io/momask"><b>MoMask</b></a>.<br>
                            <b>[July 2023]</b> Our work on 3D human motion generation via text-music intergration (TM2D) was accepted to <b>ICCV 2023</b>.<br>
                            <b>[July 2023]</b> I was awarded the J Gordin Kaplan Graduate Student Award.<br>
                            <b>[Apr  2023]</b> I was invited for a talk at <a href="https://www.youtube.com/watch?v=9toMPbHw8uE">Computer Vision Meetup.</a><br>
                            <b>[Feb  2023]</b> I was awarded the <a href="https://albertainnovates.ca/programs/graduate-student-scholarships/">Alberta Innovate Graduate Scholarship</a>.<br>
                            <b>[Dec  2022]</b> I was invited for a talk at <a href="https://www.aitime.cn/">AI TIME</a>.<br>
                            <!-- <b>[July 2022]</b> Our work on reciprocal 3D human motion and text generation was accepted to ECCV 2022.<br> -->
                            <!-- <b>[Mar  2022]</b> Our work on text-based stochastic 3D human motion generation was accepted to CVPR 2022.<br> -->
                            <!-- <b>[Mar  2022]</b> Our work on human pose and shape estimation from polarization image was accepted to TMM.<br> -->
                            <!-- <b>[Jan  2022]</b> Our work on unsupervised RGB-D saliency detection was accepted to ICLR 2022.<br> -->
                            <!-- <b>[Nov  2021]</b> Our work on action-based human video generation & animation was accepted to IJCV.<br> -->
                            <!-- <b>[Aug. 2021]</b> Our submission to IJCV received second-round decision of <>Minor Revision.<br> -->
                            <!-- <b>[Aug. 2021]</b> Our work on human pose&shape estimation from polarization images was submitted to TIP.<br> -->
                            <!-- <b>[July 2021]</b> Our work on event-based human shape estimation was accepted to ICCV 2021.<br> -->
                            <!-- <b>[Dec  2020]</b> Our work on action-based human video generation was submitted to IJCV.<br> -->
                            <!-- <b>[July 2020]</b> Our work on <a href="https://arxiv.org/pdf/2007.15240.pdf"> action-conditioned human motion generation</a> was accepted to ACM MultiMedia 2020. A 3D human motion dataset, <a href="https://ericguo5513.github.io/action-to-motion/#data" >HumanAct12</a>, which containes both fine-grained and coarse-grained action annotation of human motions was released. Check it out.<br> -->
                        </span>
                    </font>

                    <hr size="2" align="left" noshade="">

                    <h2>Publications </h2>
                    [*] indicates equal contribution. &#10013; denotes corresponding author.
                    <font face="helvetica, ariel, &#39;sans serif&#39;">
                        <table cellspacing="15">
                            <tbody>
                                <tr>
                                    <td width="30%" align=center>

                                        <img width="280" align="center" src="https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation/raw/main/figs/timeline.png" border="0">&nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>A Survey on Human Interaction Motion Generation</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://www.linkedin.com/in/kewei-sui/">Kewei Sui</a>, <a href="https://inwoohwang.me/">Inwoo Hwang</a>, <a href="https://people.mpi-inf.mpg.de/~anghosh/">Anindita Ghosh</a>, <a href="https://jianwang-cmu.github.io/">Jian Wang</a>, <b>Chuan Guo&#10013;</b><br>

                                            In ArXiv, 2025. <br>
                                            [<a href="https://arxiv.org/abs/2503.12763">Paper</a>]
                                            [<a href="https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation">Webpage</a>]
                                            <!-- [<a href="./source_files/controlMM_bib.txt">Bibtex</a>] -->
                                            <!-- [<a href="https://github.com/exitudio/ControlMM/">Code</a>] -->
                                            <!-- Place this tag where you want the button to render. -->
                                            <!-- <a class="github-button" href="https://github.com/exitudio/ControlMM" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/ControlMM on GitHub">Star</a> -->
                                            <!-- [<a href="https://github.com/gohar-malik/intermask">Demo</a>] -->

                                        </span><br>
                                    </td>
                                </tr>
                                <tr>
                                    <td width="30%" align=center>

                                        <img width="280" align="center" src="https://inwoohwang.me/images/scenemi.gif" border="0">&nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b> SceneMI: Motion In-betweening for Modeling Human-Scene Interaction</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://inwoohwang.me/">Inwoo Hwang</a>, <a href="https://zhoubinwy.github.io/">Bing Zhou&#10013;</a>, <a href="https://3d.snu.ac.kr/members/">Young Min Kim</a>, <a href="https://jianwang-cmu.github.io/">Jian Wang</a>, <b>Chuan Guo&#10013;</b><br>

                                            In ArXiv, 2025. <br>
                                            [<a href="https://arxiv.org/abs/2503.16289">Paper</a>]
                                            [<a href="http://inwoohwang.me/SceneMI">Webpage</a>]
                                            <!-- [<a href="./source_files/controlMM_bib.txt">Bibtex</a>] -->
                                            <!-- [<a href="https://github.com/exitudio/ControlMM/">Code</a>] -->
                                            <!-- Place this tag where you want the button to render. -->
                                            <!-- <a class="github-button" href="https://github.com/exitudio/ControlMM" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/ControlMM on GitHub">Star</a> -->
                                            <!-- [<a href="https://github.com/gohar-malik/intermask">Demo</a>] -->

                                        </span><br>
                                    </td>
                                </tr>
                                <tr>
                                    <td width="30%" align=center>

                                        <img width="280" align="center" src="./source_files/iclr2025_motiondreamer/jaguar_crowd.gif" border="0">&nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>MotionDreamer: One-to-More Motion Synthesis with Localized Generative Masked Transformer</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://vision-and-learning-lab-ualberta.github.io/author/yilin-wang/">Yilin Wang</a>, <b>Chuan Guo</b>, <a href="https://yxmu.foo/">Yuxuan Mu</a>, <a href="https://gohar-malik.github.io/">Muhammad Gohar Javed</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://www.ece.ualberta.ca/~hai1/">Hai Jiang</a>, <a href="https://research.snap.com/team_members/junli-cao.html">Juwei Lu</a><br>

                                            In ICLR, 2025. <br>
                                            [<a href="https://openreview.net/pdf?id=d23EVDRJ6g">Paper</a>]
                                            [<a href="https://motiondreamer.github.io/">Webpage</a>]
                                            <!-- [<a href="./source_files/controlMM_bib.txt">Bibtex</a>] -->
                                            <!-- [<a href="https://github.com/exitudio/ControlMM/">Code</a>] -->
                                            <!-- Place this tag where you want the button to render. -->
                                            <!-- <a class="github-button" href="https://github.com/exitudio/ControlMM" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/ControlMM on GitHub">Star</a> -->
                                            <!-- [<a href="https://github.com/gohar-malik/intermask">Demo</a>] -->

                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="140" align="center" src="./source_files/iclr2025_intermask/animation1_27.gif" border="0"> <img width="150" align="center" src="./source_files/iclr2025_intermask/infer_9.gif" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://gohar-malik.github.io/">Muhammad Gohar Javed</a>, <b>Chuan Guo</b>, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng</a>, <a href="https://www.ece.ualberta.ca/~xingyu/index.html">Xingyu Li</a> <br>

                                            In ICLR, 2025. <br>
                                            [<a href="https://arxiv.org/abs/2410.10010">Paper[ArXiv]</a>]
                                            [<a href="https://gohar-malik.github.io/intermask/">Webpage</a>]
                                            <!-- [<a href="https://github.com/gohar-malik/intermask">Demo</a>] -->
                                            [<a href="./source_files/intermask_bib.txt">Bibtex</a>]
                                            [<a href="https://github.com/gohar-malik/intermask">Code</a>]
                                            <a class="github-button" href="https://github.com/gohar-malik/intermask" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/InterMask on GitHub">Star</a>
                                        </span><br>
                                    </td>
                                </tr>


                                <tr>
                                    <td width="30%" align=center>

                                        <img width="280" align="center" src="https://exitudio.github.io/assets/ControlMM.gif" border="0">&nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>ControlMM: Controllable Masked Motion Generation</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://www.ekkasit.com/">Ekkasit Pinyoanuntapong</a>, <a href="https://m-usamasaleem.github.io/">Muhammad Usama Saleem</a>, <a href="https://korrawe.github.io/">Korrawe Karunratanakul</a>, <a href="https://webpages.charlotte.edu/pwang13/">Pu Wang</a>, <a href="https://havocfixer.github.io/">Hongfei Xue</a>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, <b>Chuan Guo</b>, <a href="https://research.snap.com/team_members/junli-cao.html">Junli Cao</a>, <a href="https://alanspike.github.io/">Jian Ren</a>, <a href="https://stulyakov.com/">Sergey Tulyakov</a><br>

                                            In ArXiv, 2024. <br>
                                            [<a href="https://arxiv.org/abs/2410.10780">Paper[ArXiv]</a>]
                                            [<a href="https://exitudio.github.io/ControlMM-page/">Webpage</a>]
                                            [<a href="./source_files/controlMM_bib.txt">Bibtex</a>]
                                            [<a href="https://github.com/exitudio/ControlMM/">Code</a>] <!-- Place this tag where you want the button to render. -->
                                            <!-- Place this tag where you want the button to render. -->
                                            <a class="github-button" href="https://github.com/exitudio/ControlMM" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/ControlMM on GitHub">Star</a>
                                            <!-- [<a href="https://github.com/gohar-malik/intermask">Demo</a>] -->

                                        </span><br>
                                    </td>
                                </tr>


                                <tr>
                                    <td width="30%" align=center>

                                        <img width="120" align="center" src="./source_files/eccvw2024_regiongrasp/binocular_1.gif" border="0"> &nbsp; &nbsp; &nbsp; &nbsp; <img width="120" align="center" src="./source_files/eccvw2024_regiongrasp/mug.gif" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>RegionGrasp: A Novel Task for Contact Region Controllable Hand Grasp Generation</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            Yilin Wang, <b>Chuan Guo</b>, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng</a>, <a href="https://www.ece.ualberta.ca/~hai1/">Hai Jiang</a> <br>

                                            In ECCV Workshop, 2024. <br>
                                            [<a href="https://arxiv.org/abs/2410.07995">Paper[ArXiv]</a>]
                                            [<a href="./source_files/eccvw2024_regiongrasp/eccvw_2024.txt">Bibtex</a>]
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="250" align="center" src="./source_files/eccv2024_gsd/3dpoint.gif" border="0">
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://yxmu.foo/">Yuxuan Mu</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <b>Chuan Guo</b>, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng </a><br>

                                            In ECCV, 2024. <br>
                                            [<a href="https://arxiv.org/abs/2407.04237">Paper[ArXiv]</a>]
                                            [<a href="https://yxmu.foo/GSD/">Webpage</a>]

                                            [<a href="./source_files/eccv_2024.txt">Bibtex</a>]
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="290" align="center" src="./source_files/cvpr2024_momask/demo.gif" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>MoMask: Generative Masked Modeling of 3D Human Motions</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <b>Chuan Guo*</b>, <a href="https://yxmu.foo/">Yuxuan Mu*</a>, <a href="https://scholar.google.com/citations?user=w4e-j9sAAAAJ&hl=en">Muhammad Gohar Javed*</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng </a><br>

                                            In CVPR, 2024. <br>
                                            [<a href="http://arxiv.org/abs/2312.00063">Paper[ArXiv]</a>]
                                            [<a href="https://ericguo5513.github.io/momask">Webpage</a>]
                                            [<a href="https://huggingface.co/spaces/MeYourHint/MoMask">Demo</a>]
                                            [<a href="./source_files/cvpr2024_momask/momask_2023_bib.txt">Bibtex</a>]
                                            [<a href="https://github.com/EricGuo5513/momask-codes">Code</a>]
                                            <a class="github-button" href="https://github.com/EricGuo5513/momask-codes" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/MoMask on GitHub">Star</a>
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="300" align="center" src="./source_files/iclr2024_gemostyle/demo.gif" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>Generative Human Motion Stylization in Latent Space</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <b>Chuan Guo*</b>, <a href="https://yxmu.foo/">Yuxuan Mu*</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://pdaicode.github.io/index.html">Peng Dai</a>, <a href="https://scholar.google.com/citations?user=JPUwfAMAAAAJ&hl=zh-CN">Youliang Yan </a>, <a href="https://scholar.google.ca/citations?user=Asz24wcAAAAJ&hl=en">Juwei Lu</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/">Li Cheng</a><br>

                                            In ICLR, 2024. <br>
                                            [<a href="https://openreview.net/pdf?id=daEqXJ0yZo">Paper</a>]
                                            [<a href="https://yxmu.foo/GenMoStyle/">Webpage</a>]
                                            <!-- [<a href="https://huggingface.co/spaces/MeYourHint/MoMask">Demo</a>] -->
                                            [<a href="./source_files/iclr2024_gemostyle/iclr_2024_bib.txt">Bibtex</a>]
                                            [<a href="https://github.com/Murrol/GenMoStyle-code">Code</a>]
                                            <!-- <a class="github-button" href="https://github.com/Murrol/GenMoStyle-code" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/GenMoStyle on GitHub">Star</a> -->
                                        </span><br>
                                    </td>
                                </tr>


                                <tr>
                                    <td width="30%" align=center>

                                        <img width="300" align="center" src="./source_files/aaai_2024.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://nhathoang2002.github.io/">Nhat M. Hoang</a>, Kehong Gong, <b>Chuan Guo</b>, Michael Bi Mi <br>

                                            In AAAI, 2024. <br>
                                            [<a href="https://arxiv.org/abs/2401.11115">Paper</a>]
                                            [<a href="https://nhathoang2002.github.io/MotionMix-page/">Webpage</a>]
                                            <!-- [<a href="https://huggingface.co/spaces/MeYourHint/MoMask">Demo</a>] -->
                                            [<a href="./source_files/aaai_2024_bib.txt">Bibtex</a>]
                                            [<a href="https://github.com/NhatHoang2002/MotionMix/">Code</a>]
                                            <!-- <a class="github-button" href="https://github.com/NhatHoang2002/MotionMix/" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/MotionMix on GitHub">Star</a> -->
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="300" align="center" src="./source_files/iccv_2023.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            Kehong Gong, <a href="https://dongzelian.com/">Dongze Lian</a>, <a href="https://hchang95.github.io/">Heng Chang</a>, <b>Chuan Guo </b>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, Zhihang Jiang, <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang </a><br>

                                            In ICCV, 2023. <br>
                                            [<a href="http://arxiv.org/abs/2304.02419">Paper[ArXiv]</a>]
                                            [<a href="https://garfield-kh.github.io/TM2D/">Webpage</a>]
                                            [<a href="https://garfield-kh.github.io/TM2D/Bibtex.txt">Bibtex</a>]
                                            [<a href="https://github.com/Garfield-kh/TM2D">Code</a>]
                                            <a class="github-button" href="https://github.com/Garfield-kh/TM2D" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/TM2D on GitHub">Star</a>
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="300" align="center" src="./source_files/eccv_2022.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <b>Chuan Guo </b>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                            In ECCV, 2022. <br>
                                            [<a href="http://arxiv.org/abs/2207.01696">Paper[ArXiv]</a>]
                                            [<a href="https://ericguo5513.github.io/TM2T/">Webpage</a>]
                                            [<a href="https://www.youtube.com/watch?v=k7BRyxAxsfQ">Video</a>]
                                            [<a href="https://github.com/EricGuo5513/HumanML3D">Data</a>]
                                            [<a href="https://ericguo5513.github.io/TM2T/Bibtex.txt">Bibtex</a>]
                                            [<a href="https://medium.com/@weihsinyeh168/paper-reading-tm2t-stochastic-and-tokenized-modeling-for-the-reciprocal-generation-of-3d-human-eb089ef02d48"> Paper Reading</a>]
                                            [<a href="https://github.com/EricGuo5513/TM2T">Code</a>]
                                            <a class="github-button" href="https://github.com/EricGuo5513/TM2T" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/TM2T on GitHub">Star</a>
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="300" align="center" src="./source_files/cvpr_2022.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>Generating Diverse and Natural 3D Human Motions from Text</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <b>Chuan Guo </b>, <a href="https://jimmyzou.github.io/">Shihao Zou</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <a href="https://vision-and-learning-lab-ualberta.github.io/author/wei-ji/">Wei Ji</a>, <a href="https://www.ece.ualberta.ca/~xingyu/index.html"> Xingyu Li</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                            In CVPR, 2022. <br>
                                            [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf">Paper</a>]
                                            [<a href="https://ericguo5513.github.io/text-to-motion/">Webpage</a>]
                                            [<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Guo_Generating_Diverse_and_CVPR_2022_supplemental.pdf">Supp</a>]
                                            [<a href="https://www.youtube.com/watch?v=085mBtMeZpg">Video</a>]
                                            [<a href="https://ericguo5513.github.io/text-to-motion/Bibtex.txt">Bibtex</a>]
                                            [<a href="https://github.com/EricGuo5513/text-to-motion">Code</a>]
                                            <a class="github-button" href="https://github.com/EricGuo5513/text-to-motion" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/text-to-motion on GitHub">Star</a>
                                            [<a href="https://github.com/EricGuo5513/HumanML3D">Data</a>]
                                            <a class="github-button" href="https://github.com/EricGuo5513/HumanML3D" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/HumanML3D on GitHub">Star</a>
                                        </span><br>
                                    </td>
                                </tr>

                                <tr>
                                    <td width="30%" align=center>

                                        <img width="250" align="center" src="./source_files/tmm_2022.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>Human Pose and Shape Estimation from Single Polarization Images</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                            <a href="https://jimmyzou.github.io/">Shihao Zou</a>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <a href="https://yi-ming-qian.github.io/">Yiming Qian</a>, <b>Chuan Guo</b>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                            In TMM, 2022. <br>
                                            [<a href="https://ieeexplore.ieee.org/document/9743297">Paper</a>]
                                            [<a href="https://arxiv.org/pdf/2108.06834.pdf">Paper(ArXiv)</a>]
                                            [<a href="./source_files/tmm_2022_bib.txt">Bibtex</a>]
                                        </span><br>
                                    </td>
                                </tr>


                                <tr>
                                    <td width="30%" align=center>

                                        <img width="270" align="center" src="./source_files/iclr_2022.png" border="0"> &nbsp;
                                    </td>

                                    <td>
                                        <span style="font-size: 12pt;">
                                            <b>Promoting Saliency from Depth: Deep Unsupervised RGB-D Saliency Detection.</b> 
                                        </span><br>
                                        <span style="font-size: 10pt;">
                                         <a href="https://vision-and-learning-lab-ualberta.github.io/author/wei-ji/">  Wei Ji</a>, <a href="https://vision-and-learning-lab-ualberta.github.io/author/jingjing-li/"> Jingjing Li</a>, Qi Bi, <b>Chuan Guo</b>, Jie Liu, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                         In ICLR, 2022. <br>
                                         [<a href="https://openreview.net/pdf?id=BZnnMbt0pW">Paper</a>]
                                         [<a href="https://github.com/jiwei0921/DSU">Code</a>]
                                         [<a href="./source_files/iclr_2022_bib.txt">Bibtex</a>]
                                     </span><br>
                                 </td>
                             </tr>

                             <tr>
                                <td width="30%" align=center>

                                    <img width="130" align="center" src="./source_files/ijcv2022_action2video/demo1.gif" border="0"> <img width="160" align="center" src="./source_files/ijcv2022_action2video/demo2.gif" border="0"> &nbsp;
                                </td>

                                <td>
                                    <span style="font-size: 12pt;">
                                        <b>Action2video: Generating Videos of Human 3D Actions</b> 
                                    </span><br>
                                    <span style="font-size: 10pt;">
                                     <b>Chuan Guo</b>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, Xinshuang Liu, <a href="https://jimmyzou.github.io/">Shihao Zou</a>, <a href="http://www.cs.mun.ca/~gong/"> Minglun Gong</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                     In IJCV, 2021. <br>
                                     [<a href="https://link.springer.com/article/10.1007/s11263-021-01550-z">Paper</a>]
                                     [<a href="https://arxiv.org/pdf/2111.06925.pdf">Paper(ArXiv)</a>]
                                     [<a href="https://youtu.be/DiPDd1vLGFc">Video</a>]
                                     [<a href="./source_files/ijcv2022_action2video/ijcv_2021_bib.txt">Bibtex</a>]
                                 </span><br>
                             </td>
                         </tr>

                         <tr>
                            <td width="30%" align=center>
                                <img width="250" align="center" src="./source_files/iccv_2021.png" border="0"> &nbsp;
                            </td>

                            <td>
                                <span style="font-size: 12pt;">
                                    <b>EventHPE: Event-based 3D Human Pose and Shape Estimation</b> 
                                </span><br>
                                <span style="font-size: 10pt;">
                                    <a href="https://jimmyzou.github.io/">Shihao Zou</a>, <b>Chuan Guo</b>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, Pengyu Wang, Xiaoqin Hu, Shoushun Chen, <a href="http://www.cs.mun.ca/~gong/"> Minglun Gong</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                    In ICCV, 2021. <br>
                                    [<a href="https://arxiv.org/pdf/2108.06819.pdf">Paper</a>]
                                    <!-- [<a href="https://ericguo5513.github.io/action-to-motion/">Webpage</a>] -->
                                    [<a href="https://github.com/JimmyZou/EventHPE">Code</a>]
                                    <!-- [<a href="https://www.youtube.com/watch?v=eDzN3mhNdeo">Video</a>] -->
                                    [<a href="./source_files/iccv_2021_bib.txt">Bibtex</a>]
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="30%" align=center>
                                <img width="300" align="center" src="https://ericguo5513.github.io/action-to-motion/website/teaser.png" border="0"> &nbsp;
                            </td>

                            <td>
                                <span style="font-size: 12pt;">
                                    <b>Action2Motion: Conditioned Generation of 3D Human Motions</b> 
                                </span><br>
                                <span style="font-size: 10pt;">
                                    <b>Chuan Guo</b>, <a href="https://sites.google.com/site/xinxinzuohome/home">Xinxin Zuo</a>, <a href="https://sites.google.com/site/senwang1312home/">Sen Wang</a>, <a href="https://jimmyzou.github.io/">Shihao Zou</a>, Qingyao Sun, Annan Deng, <a href="http://www.cs.mun.ca/~gong/"> Minglun Gong</a>, <a href="https://www.ece.ualberta.ca/~lcheng5/"> Li Cheng</a><br>
                                    In MultiMedia, 2020. <br>
                                    [<a href="https://arxiv.org/pdf/2007.15240.pdf">Paper</a>]
                                    [<a href="https://ericguo5513.github.io/action-to-motion/">Webpage</a>]
                                    [<a href="https://www.youtube.com/watch?v=eDzN3mhNdeo">Video</a>]
                                    [<a href="https://ericguo5513.github.io/action-to-motion/website/bibtex.txt">Bibtex</a>]
                                    [<a href="https://github.com/EricGuo5513/action-to-motion">Code</a>]
                                    <a class="github-button" href="https://github.com/EricGuo5513/action-to-motion" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true" aria-label="Star exitudio/action-to-motion on GitHub">Star</a>
                                </span><br>
                            </td>
                        </tr>

                        <tr>
                            <td width="30%" align=left>
                                <img height="100" align="center" src="./source_files/rumor_1.png" border="0">&nbsp;&nbsp;&nbsp;&nbsp;
                                <img height="100" align="center" src="./source_files/rumor_2.png" border="0">
                            </td>
                            <td>
                                <span style="font-size: 12pt;">
                                    <b> DEAN: Learning Dual Emotion for Fake News Detection on Social Media</b></span><br>
                                    <span style="font-size: 10pt;">
                                        <b>Chuan Guo</b>, Juan Cao, Xueyao Zhang, <a href="http://www.cs.iit.edu/~kshu/">Kai Shu</a>, <a href="http://www.public.asu.edu/~huanliu/">Huan Liu</a><br>
                                        In ArXiv, 2019. <br>
                                        <!-- [<a href="">Paper</a>] -->
                                        [<a href="https://arxiv.org/pdf/1903.01728.pdf">Paper</a>]
                                    </span><br>
                                </td>
                            </tr>



                        </tbody>
                    </table>
                </font>


                <font face="helvetica, ariel, &#39;sans serif&#39;">

                    <h2>Academic Activities </h2>
                    <span style="font-size: 12pt;">Organizer of  <a href="https://humogen.github.io/"> Human Motion Generation (HuMoGen)</a> Workshop on CVPR 2024-2025.</span><br><br>
                    <span style="font-size: 12pt;">
                        Reviewer for ICCV 2023, 2025, SIGGRAPH 2024-25, ICML 2024-2025, CVPR 2023-25, Eurographics 2022,2025, ICLR 2024-2025, AAAI 2023-2025, NeurIPS 2023-24, SIGGRAPH Asia 2024,  ECCV 2024, ICML 2024, MultiMedia 2024,  ACCV 2022, EMNLP 2021, ACML 2020-2021<br>
                        Reviewer for Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
                        Reviewer for IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)<br>
                        Reviewer for IEEE Robotics and Automation Letters (RA-L) <br>
                        Reviewer for IEEE Transactions on Multimedia (TMM)<br>
                        Reviewer for IEEE Transactions on Neural Networks and Learning Systems (TNNLS)<br>
                        Reviewer for Pattern Recognition (PR)<br>
                        Reviewer for Machine Learning<br>

                    </span>


                    <h2>Selected Honors & Awards </h2>
                    <span style="font-size: 12pt;">
                        <b>J Gordin Kaplan Graduate Student Award</b>, University of Alberta, 2023<br>
                        <b>Alberta Innovate Graduate Scholarship</b>, Alberta Province, 2023<br>
                        <b>Alberta Graduate Excellence Scholarship</b>, Alberta Province, 2021<br>
                        <b>Qihoo 360 Scholarship</b>, Qihoo 360 Technology Co. Ltd., 2016<br>
                        <b>Undergraduate National Scholarship</b>, Chinese Ministry of Education, 2015<br>
                        <b>Excellent Student of Jilin University</b>, Jilin University, 2015<br>
                        <br>
                        <b>ECCV 2022 Student Travel Grant</b> <br>
                        <b>CVPR 2022 Student Travel Grant</b> <br>
                        <b>Outstanding National Undergraduate Innovative Training Project</b>, Jilin University, 2016<br>
                        <b>First-Prize in Jilin Provincial Mathematical Contest in Modeling</b>, Jilin Provincial Miniistry of Education, 2015<br>
                    </span>
                </font>

                <h3>Misc </h3>
                <font face="helvetica, ariel, &#39;sans serif&#39;">

                    <span style="font-size: 10pt;">
                        I'm from Chengdu, China.<br>
                        This website template is borrowed from <a href="https://richzhang.github.io/">here</a>.<br>
                        <!-- Late updated on Dec 18, 2021. -->
                    </span>
                </font>
            </td>
        </tr>
        <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=OKxgG6Tz32JT5o9d7QX9toZSkFtz65EOAQd-mkJRAhk"></script> -->

    </tbody>
</table>

    <!-- <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-75897335-1', 'auto');
      ga('send', 'pageview');
    </script>  -->

        <!-- <a href="https://clustrmaps.com/site/1c3yc"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=OKxgG6Tz32JT5o9d7QX9toZSkFtz65EOAQd-mkJRAhk&cl=ffffff" /></a> -->
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=OKxgG6Tz32JT5o9d7QX9toZSkFtz65EOAQd-mkJRAhk&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
    </body>

</html>
